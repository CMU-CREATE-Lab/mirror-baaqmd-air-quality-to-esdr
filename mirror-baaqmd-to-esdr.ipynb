{
 "metadata": {
  "name": "",
  "signature": "sha256:935229d026e2817110cbf351a569db23d25a6ed1725aad48caef88a282600714"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Currently we have 113 sites in sites.json from create-sites-list.\n",
      "\n",
      "But http://www.baaqmd.gov/about-air-quality/current-air-quality/air-monitoring-data?DataViewFormat=daily&DataView=aqi&StartDate=11/7/2016&ParameterId=316 only lists 36 sites in the drop-down.  Why aren't we getting data from all 113?\n",
      "\n",
      "- upload historical data \n",
      "- auto-upload recent data (daily every 5 mins, 2, 4, 8, 16, 32, 64\n",
      "- 1/2 + 1/4 + 1/8 ...\n",
      "- 1/3 + 2/9 + 4/27 ...\n",
      "- 1/4 + 3/16 + 9/64 ...\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime, re, time, urllib2\n",
      "from dateutil import tz\n",
      "from dateutil import rrule\n",
      "\n",
      "# scrapy web scraper\n",
      "# To install, run\n",
      "# !pip install scrapy\n",
      "# then restart kernel\n",
      "# If you see an error about zope version, double-check that you restarted the kernel\n",
      "from scrapy.selector import Selector\n",
      "\n",
      "def exec_ipynb(url):\n",
      "    import json, re, urllib2\n",
      "    nb = (urllib2.urlopen(url) if re.match(r'https?:', url) else open(url)).read()\n",
      "    exec '\\n'.join([''.join(cell['input']) for cell in json.loads(nb)['worksheets'][0]['cells'] if cell['cell_type'] == 'code']) in globals()\n",
      "\n",
      "exec_ipynb('python-utils/esdr-library.ipynb')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First time uploading, create a new client like so:\n",
      "\n",
      "# Esdr.save_client('esdr-auth-baaqm-uploader.json', 'BAAQMD uploader 2 for timemachine1')\n",
      "\n",
      "# and then follow the directions it prints, which include visiting esdr.cmucreatelab.org and creating\n",
      "# a client with given parameters, and also editing esdr-auth-baaqm-uploader.json to include your\n",
      "# username and password\n",
      "\n",
      "# Do not add esdr-auth-baaqm-uploader.json to the git repo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "esdr = Esdr('esdr-auth-baaqm-uploader.json')\n",
      "product = esdr.get_or_create_product('BAAQMD', 'BAAQMD', 'Sensor network run by Bay Area Air Quality Management District')\n",
      "product"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "{u'created': u'2015-05-28T16:45:17.000Z',\n",
        " u'creatorUserId': 3,\n",
        " u'defaultChannelSpecs': {},\n",
        " u'description': u'Sensor network run by Bay Area Air Quality Management District',\n",
        " u'id': 35,\n",
        " u'modified': u'2015-05-28T16:45:17.000Z',\n",
        " u'name': u'BAAQMD',\n",
        " u'prettyName': u'BAAQMD',\n",
        " u'vendor': u'BAAQMD'}"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baaqmd_tz = tz.tzoffset(\"PST\", -8 * 3600)\n",
      "\n",
      "# BAAQMD always reports in Pacific Standard Time,\n",
      "# even when the Bay Area observes Pacific Daylight Time during the summer.\n",
      "\n",
      "# In other words, BAAQMD times look correct in the winter, but are appear\n",
      "# to be one hour behind people's clocks during the summer.\n",
      "\n",
      "# For example, if during the summer, if\n",
      "# BAAQMD reports 3pm Pacific Standard Time,\n",
      "# that would correspond to 4pm Pacific Daylight Time.\n",
      "\n",
      "# Test that timezone has offset 8 hours (PST) both during summer and winter\n",
      "\n",
      "# Test this timezone.  Confirm epoch time of midnight 1/1/70 was 5 hours\n",
      "date = datetime.datetime.strptime('1/1/1970 00:00', '%m/%d/%Y %H:%M').replace(tzinfo=baaqmd_tz)\n",
      "epoch = (date - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
      "if epoch != 8 * 3600:\n",
      "    raise Exception(\"Error in timezone\")\n",
      "\n",
      "date = datetime.datetime.strptime('7/1/1970 00:00', '%m/%d/%Y %H:%M').replace(tzinfo=baaqmd_tz)\n",
      "epoch = (date - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
      "if epoch % 86400 != 8 * 3600:\n",
      "    raise Exception(\"Error in timezone\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gentleFetch(url):\n",
      "    for i in range(0, 10):\n",
      "        try:\n",
      "            return urllib2.urlopen(url).read()\n",
      "        except Exception, e:\n",
      "            # if it's a HTTP error with code 404, don't retry\n",
      "            try:\n",
      "                if e.code == 404:\n",
      "                    raise\n",
      "            except:\n",
      "                pass\n",
      "            print 'Error %s fetching %s, retrying' % (e, url)\n",
      "            time.sleep(15)\n",
      "            \n",
      "    return urllib2.urlopen(url).read()\n",
      "\n",
      "def makeIdentifier(prettyName):\n",
      "    return re.sub('\\W+', '_', prettyName).strip('_')\n",
      "\n",
      "def getStations():\n",
      "    url = \"http://www.baaqmd.gov/about-air-quality/current-air-quality/air-monitoring-data?DataViewFormat=daily\"\n",
      "    body = gentleFetch(url)\n",
      "    selector = Selector(text = body)\n",
      "    ids = selector.xpath('//select[contains(@name, \"StationSelect\")]/option/@value').extract()\n",
      "    names = selector.xpath('//select[contains(@name, \"StationSelect\")]/option/text()').extract()\n",
      "    sites = dict(zip(ids, names))\n",
      "    if '' in sites:\n",
      "        del sites['']\n",
      "    #ret = sorted([id for id in ids if re.match('\\d+', id)])\n",
      "    #print 'All station IDs: %s' % ', '.join(ret)\n",
      "    return sites\n",
      "\n",
      "stations = getStations()\n",
      "print \"There are %d stations\" % len(stations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 36 stations\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getStationInfo(stationId):\n",
      "    # First look in sites.json\n",
      "    stations = json.load(open('sites.json'))\n",
      "    if stationId in stations:\n",
      "        return stations[stationId]\n",
      "    \n",
      "    print 'Cannot find %s in sites.json' % stationId\n",
      "\n",
      "    html = None\n",
      "    for stationType in ['tec', 'met']:\n",
      "        try:\n",
      "            url = 'http://hank.baaqmd.gov/tec/maps/%ssites%s.htm' % (stationType, stationId)\n",
      "            html = gentleFetch(url)\n",
      "        except urllib2.HTTPError:\n",
      "            pass\n",
      "    if not html:\n",
      "        print 'Cannot scrape data for station %s' % stationId\n",
      "        raise Exception('Cannot scrape data for station %s' % stationId)\n",
      "    selector = Selector(text = html)\n",
      "    title = selector.xpath('//title/text()').extract()[0]\n",
      "    name = re.search(r\":(.*)\", title).group(1).strip()\n",
      "    name = name.replace(u'\\xa0', ' ')\n",
      "    # For some reason, the page shows longitude as positive instead of negative\n",
      "    return {\n",
      "        'name': name,\n",
      "        'longitude': -abs(float(re.search(r'Longitude.*?(-?\\d+\\.\\d+)', html).group(1))),\n",
      "        'latitude': float(re.search(r'Latitude.*?(-?\\d+\\.\\d+)', html).group(1))\n",
      "    }\n",
      "\n",
      "def printCombinedStationInfo():\n",
      "    for (id, name) in getStations().iteritems():\n",
      "        try:\n",
      "            info = getStationInfo(id)\n",
      "            print '%s\\t%s\\t%s\\t%s\\t%s' % (id, name, info['name'], info['latitude'], info['longitude'])\n",
      "        except:\n",
      "            print '%s\\t%s' % (id, name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getStationFeed(stationId, provisionalStationName):\n",
      "    stationInfo = {}\n",
      "    device = esdr.get_device_by_serial_number(product, stationId)\n",
      "    if not device:\n",
      "        try:\n",
      "            stationInfo = getStationInfo(stationId)\n",
      "        except:\n",
      "            pass\n",
      "        stationName = stationInfo['name'] if 'name' in stationInfo else provisionalStationName\n",
      "        esdr.create_device(product, stationId, name=stationName)\n",
      "        device = esdr.get_device_by_serial_number(product, stationId)\n",
      "\n",
      "    feed = esdr.get_feed(device)\n",
      "    if not feed:\n",
      "        try:\n",
      "            stationInfo = getStationInfo(stationId)\n",
      "        except:\n",
      "            pass\n",
      "        lat = None\n",
      "        lon = None\n",
      "        if 'latitude' in stationInfo:\n",
      "            lat = stationInfo['latitude']\n",
      "            lon = stationInfo['longitude']\n",
      "        esdr.create_feed(device, lat=lat, lon=lon)\n",
      "        feed = esdr.get_feed(device)\n",
      "    \n",
      "    return feed\n",
      "\n",
      "#getStationFeed('1025', 'Oakland East')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def uploadStation(stationId, provisionalStationName, date):\n",
      "    formattedDate = date.strftime('%m/%d/%Y')\n",
      "    channel_names = []\n",
      "    upload_data = []\n",
      "    for hour in range(0, 24):\n",
      "        # Set minute=30 to use timestamp in middle of hourlong sampling period\n",
      "        time = date.replace(hour=hour, minute=30, second=0, microsecond=0, tzinfo=baaqmd_tz)\n",
      "        epoch_time = (time - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
      "        upload_data.append([epoch_time])\n",
      "    \n",
      "    for paramType in ['tech', 'met']:\n",
      "        \n",
      "        url = 'http://www.baaqmd.gov/about-air-quality/current-air-quality/air-monitoring-data?DataViewFormat=daily&DataView=%s&StartDate=%s&ParameterId=&StationId=%s' % (paramType, formattedDate, stationId)\n",
      "        try:\n",
      "            body = gentleFetch(url)\n",
      "        except urllib2.HTTPError:\n",
      "            print 'Error fetching %s' % url\n",
      "            continue\n",
      "        selector = Selector(text = body)\n",
      "        params = selector.xpath('//tr[count(td)>=25]')\n",
      "        for param in params:\n",
      "            param_name = param.xpath('td/a/text()')[0].extract()\n",
      "            channel_names.append(makeIdentifier(param_name))\n",
      "            columns = param.xpath('td/div[@class=\"cData\"]')\n",
      "            for hour in range(0, 24):\n",
      "                column = columns[hour]\n",
      "                if len(column.xpath('text()')):\n",
      "                    upload_data[hour].append(float(column.xpath('text()').extract()[0]))\n",
      "                else:\n",
      "                    upload_data[hour].append(False) # False deletes sample at same time datastore\n",
      "\n",
      "    feed = getStationFeed(stationId, provisionalStationName)\n",
      "\n",
      "    esdr.upload(feed, {\n",
      "        'channel_names': channel_names,\n",
      "        'data': upload_data\n",
      "    })\n",
      "    print 'Uploaded %d channels from %s to %s:%s' % (len(channel_names), date.strftime('%m/%d/%Y'), stationId, provisionalStationName)\n",
      "\n",
      "def uploadStations(date):\n",
      "    for (stationId, provisionalStationName) in getStations().iteritems():\n",
      "        uploadStation(stationId, provisionalStationName, date)\n",
      "\n",
      "#uploadStations(datetime.datetime(2015, 5, 19)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Link to viewing Pt Richmond H2S to observe what's been uploaded so far](https://esdr.cmucreatelab.org/browse/#channels=4852.Hydrogen_Sulfide_H2S&time=1336340011.4569557,1504950527.7471123&zoom=13&center=37.94193832885333,-122.36425476074216&cursor=1478533784.615&search=baaqmd)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test:  upload all data from a single day\n",
      "uploadStations(datetime.datetime(2016, 11, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Uploaded 6 channels from 11/05/2016 to 5011:San Francisco - Arkansas St.\n",
        "Uploaded 5 channels from 11/05/2016 to 2907:Pt. San Pablo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 6 channels from 11/05/2016 to 3005:San Rafael"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 7 channels from 11/05/2016 to 7032:San Jose - Jackson St."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 6 channels from 11/05/2016 to 6901:San Carlos"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 6 channels from 11/05/2016 to 7039:San Jose Fwy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 8 channels from 11/05/2016 to 7015:Gilroy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 1 channels from 11/05/2016 to 3009:Forest Knolls"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 5 channels from 11/05/2016 to 1903:Chabot"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 6 channels from 11/05/2016 to 6004:Redwood City"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 6 channels from 11/05/2016 to 4001:Napa"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 9 channels from 11/05/2016 to 2037:San Ramon"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 13 channels from 11/05/2016 to 2036:Concord"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 7 channels from 11/05/2016 to 2035:San Pablo - Rumrill"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 1 channels from 11/05/2016 to 2034:Rodeo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 2 channels from 11/05/2016 to 2019:Richmond - 7th St"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 1 channels from 11/05/2016 to 2014:Martinez - Jones St"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 7 channels from 11/05/2016 to 5801:San Francisco STP"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 7 channels from 11/05/2016 to 1030:Berkeley Aquatic Park"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 6 channels from 11/05/2016 to 8901:Rio Vista"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 1 channels from 11/05/2016 to 2013:Pt. Richmond"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 8 channels from 11/05/2016 to 1027:Oakland West"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 7 channels from 11/05/2016 to 7022:San Martin-Airport"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 7 channels from 11/05/2016 to 9005:Sebastopol"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 1 channels from 11/05/2016 to 1015:Hayward"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 14 channels from 11/05/2016 to 8004:Vallejo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 7 channels from 11/05/2016 to 8007:Fairfield - Chadbourne Rd."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 1 channels from 11/05/2016 to 2017:Crockett - Kendall Ave"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 1 channels from 11/05/2016 to 7006:Los Gatos"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 6 channels from 11/05/2016 to 1025:Oakland East"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 8 channels from 11/05/2016 to 1805:Berkeley Lab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 6 channels from 11/05/2016 to 9903:Sonoma Baylands"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 14 channels from 11/05/2016 to 1023:Livermore - Rincon Ave."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 6 channels from 11/05/2016 to 1029:Laney College Fwy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 9 channels from 11/05/2016 to 1028:Patterson Pass"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Uploaded 13 channels from 11/05/2016 to 2021:Bethel Island"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#8 28 2012\n",
      "for date in reversed(list(rrule.rrule(rrule.DAILY, dtstart=datetime.datetime(2015, 5, 28), until=datetime.datetime(2015, 8, 13)))):\n",
      "    uploadStations(date)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Unneeded\n",
      "#\n",
      "#def getParams(paramType):\n",
      "#    # paramType is tech or met\n",
      "#    url = \"http://www.baaqmd.gov/Divisions/Technical-Services/Ambient-Air-Monitoring/Air-Monitoring-Data.aspx?DataViewFormat=daily&DataView=%s\" % paramType\n",
      "#    body = gentleFetch(url)\n",
      "#    selector = Selector(text = body)\n",
      "#    ids = selector.xpath('//select[contains(@name, \"MeasurementSelect\")]/option/@value').extract()\n",
      "#    names = selector.xpath('//select[contains(@name, \"MeasurementSelect\")]/option/text()').extract()\n",
      "#    parameters = dict(zip(ids, names))\n",
      "#    print 'Found %s params from %s: %s' % (paramType, url, parameters)\n",
      "#    return parameters\n",
      "\n",
      "# p=getParams('tech')\n",
      "# getParams('met')\n",
      "\n",
      "#def getParamFromAllSites(paramType, paramId, date):\n",
      "#    url = 'http://www.baaqmd.gov/Divisions/Technical-Services/Ambient-Air-Monitoring/Air-Monitoring-Data.aspx?DataViewFormat=daily&DataView=%s&StartDate=%s&ParameterId=%s' % (paramType, date, paramId)\n",
      "#    body = gentleFetch(url)\n",
      "#    selector = Selector(text = body)\n",
      "#    sites = selector.xpath('//tr[count(td)>=25]')\n",
      "#    for site in sites:\n",
      "#        sitename = site.xpath('td/a/text()')[0].extract()\n",
      "#        values = [float(value) for value in site.xpath('td/div[@class=\"cData\"]/text()').extract()]\n",
      "#        print sitename, values\n",
      "\n",
      "# getParamFromAllSites('tech', '59', '5/22/2015')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}